{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating user data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Users:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Users: 100%|██████████| 1000/1000 [07:16<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating gig data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gigs: 100%|██████████| 3000/3000 [00:00<00:00, 3629.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving users to CSV...\n",
      "Saving gigs to CSV...\n",
      "Data generation complete with constraints!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import bcrypt\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "fake = Faker('en_IN')\n",
    "\n",
    "def hash_password(password):\n",
    "    return bcrypt.hashpw(password.encode(), bcrypt.gensalt()).decode()\n",
    "\n",
    "# Platform constraints data\n",
    "platforms_data = [\n",
    "    {\"platform_id\": 1, \"platform_name\": \"Swiggy\", \"platform_type\": \"Delivery\"},\n",
    "    {\"platform_id\": 2, \"platform_name\": \"Zomato\", \"platform_type\": \"Delivery\"},\n",
    "    {\"platform_id\": 3, \"platform_name\": \"Uber\", \"platform_type\": \"Driver\"},\n",
    "    {\"platform_id\": 4, \"platform_name\": \"Ola\", \"platform_type\": \"Driver\"},\n",
    "    {\"platform_id\": 5, \"platform_name\": \"Fiverr\", \"platform_type\": \"Freelance\"},\n",
    "    {\"platform_id\": 6, \"platform_name\": \"InDrive\", \"platform_type\": \"Driver\"},\n",
    "    {\"platform_id\": 7, \"platform_name\": \"Blinkit\", \"platform_type\": \"Delivery\"},\n",
    "    {\"platform_id\": 8, \"platform_name\": \"Zepto\", \"platform_type\": \"Delivery\"},\n",
    "    {\"platform_id\": 9, \"platform_name\": \"MagicPin\", \"platform_type\": \"Delivery\"}\n",
    "]\n",
    "\n",
    "def generate_user_data(user_id):\n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"first_name\": fake.first_name(),\n",
    "        \"last_name\": fake.last_name(),\n",
    "        \"email\": fake.email(),\n",
    "        \"password_hash\": hash_password(\"password123\"),\n",
    "        \"phone_number\": fake.phone_number(),\n",
    "        \"location_lat\": random.uniform(18.5, 19.5),\n",
    "        \"location_lng\": random.uniform(72.7, 73.0),\n",
    "        \"user_type\": random.choice(['Driver', 'Delivery', 'Freelancer']),\n",
    "        \"rating\": round(random.uniform(1, 5), 1),\n",
    "        \"availability_status\": random.choice(['Available', 'Busy', 'Offline']),\n",
    "        \"total_earnings\": round(random.uniform(1000, 100000), 2),\n",
    "        \"preferred_gig_type\": random.choice(['Delivery', 'Driving', 'Freelance'])\n",
    "    }\n",
    "\n",
    "def generate_gig_data(gig_id, user_id):\n",
    "    gig_type = random.choice(['Delivery', 'Driver', 'Freelance'])\n",
    "    platform = random.choice([p for p in platforms_data if p[\"platform_type\"] == gig_type])\n",
    "    start_time = fake.date_time_this_month()\n",
    "    end_time = start_time + timedelta(hours=random.randint(1, 5))\n",
    "    return {\n",
    "        \"gig_id\": gig_id,\n",
    "        \"gig_type\": platform[\"platform_type\"],\n",
    "        \"platform\": platform[\"platform_name\"],\n",
    "        \"location_lat\": random.uniform(18.5, 19.5),\n",
    "        \"location_lng\": random.uniform(72.7, 73.0),\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"earnings\": round(random.uniform(100, 5000), 2),\n",
    "        \"gig_status\": random.choice(['Completed', 'In-progress', 'Cancelled']),\n",
    "        \"demand_score\": round(random.uniform(0.5, 1.0), 2),\n",
    "        \"travel_distance\": round(random.uniform(1, 15), 1),\n",
    "        \"estimated_time_to_complete\": random.randint(10, 180),\n",
    "        \"user_id\": user_id\n",
    "    }\n",
    "\n",
    "def save_to_csv(data, filename, fieldnames):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Generate data with progress bars\n",
    "print(\"Generating user data...\")\n",
    "users = [generate_user_data(user_id) for user_id in tqdm(range(1, 1001), desc=\"Users\")]\n",
    "\n",
    "print(\"Generating gig data...\")\n",
    "gigs = [generate_gig_data(gig_id, random.randint(1, 1000)) for gig_id in tqdm(range(1, 3001), desc=\"Gigs\")]\n",
    "\n",
    "# Save to CSV\n",
    "print(\"Saving users to CSV...\")\n",
    "save_to_csv(users, \"Users.csv\", list(users[0].keys()))\n",
    "\n",
    "print(\"Saving gigs to CSV...\")\n",
    "save_to_csv(gigs, \"Gigs.csv\", list(gigs[0].keys()))\n",
    "\n",
    "print(\"Data generation complete with constraints!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "username='arora707vicky'\n",
    "password='FxQgGE8zM07E0yS7'\n",
    "cluster_url=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Replace <username>, <password>, and <cluster-url> with your details\n",
    "client = MongoClient(f\"mongodb+srv://arora707vicky:{password}@gigs.7jzwe.mongodb.net/?retryWrites=true&w=majority&appName=gigs\")\n",
    "db = client.AggregatorDB\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the database and collection\n",
    "db = client['gigs']  # Use the database name you want to access\n",
    "collection = db['Gigs']  # Use the collection you want to access\n",
    "\n",
    "# Example query: Find all documents in the collection\n",
    "for document in collection.find():\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data cleared from collections.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB Atlas connection string\n",
    "uri = f\"mongodb+srv://arora707vicky:{password}@gigs.7jzwe.mongodb.net/?retryWrites=true&w=majority&appName=gigs\"\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Connect to the database and collection\n",
    "db = client['Gigs']  # Your database name\n",
    "\n",
    "# Delete all data from Users collection\n",
    "db.Users.delete_many({})\n",
    "\n",
    "# Delete all data from Gigs collection\n",
    "db.Gigs.delete_many({})\n",
    "\n",
    "print(\"All data cleared from collections.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# MongoDB Atlas connection string\n",
    "uri = f\"mongodb+srv://arora707vicky:{password}@gigs.7jzwe.mongodb.net/?retryWrites=true&w=majority&appName=gigs\"\n",
    "client = MongoClient(uri)\n",
    "\n",
    "# Connect to the database and collection\n",
    "db = client['Gigs']  # Your database name\n",
    "users_collection = db['users']  # Replace with your collection name\n",
    "gigs_collection = db['gigs']\n",
    "platforms_collection = db['platforms']\n",
    "\n",
    "# Read CSV files using pandas\n",
    "users_df = pd.read_csv('Users.csv')  # Path to your Users.csv file\n",
    "gigs_df = pd.read_csv('Gigs.csv')  # Path to your Gigs.csv file\n",
    "platforms_df = pd.read_csv('Platforms.csv')  # Path to your Platforms.csv file\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "users_data = users_df.to_dict(orient='records')\n",
    "gigs_data = gigs_df.to_dict(orient='records')\n",
    "platforms_data = platforms_df.to_dict(orient='records')\n",
    "\n",
    "# Insert data into MongoDB collections\n",
    "users_collection.insert_many(users_data)\n",
    "gigs_collection.insert_many(gigs_data)\n",
    "platforms_collection.insert_many(platforms_data)\n",
    "\n",
    "print(\"Data inserted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating platform analytics data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Platform Analytics: 9it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving platform analytics to CSV...\n",
      "Additional data generation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta\n",
    "import bcrypt\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "fake = Faker('en_IN')\n",
    "\n",
    "# Platform constraints data\n",
    "platforms_data = [\n",
    "    {\"platform_id\": 1, \"platform_name\": \"Swiggy\", \"platform_type\": \"Delivery\"},\n",
    "    {\"platform_id\": 2, \"platform_name\": \"Zomato\", \"platform_type\": \"Delivery\"},\n",
    "    {\"platform_id\": 3, \"platform_name\": \"Uber\", \"platform_type\": \"Driver\"},\n",
    "    {\"platform_id\": 4, \"platform_name\": \"Ola\", \"platform_type\": \"Driver\"},\n",
    "    {\"platform_id\": 5, \"platform_name\": \"Fiverr\", \"platform_type\": \"Freelance\"},\n",
    "    {\"platform_id\": 6, \"platform_name\": \"InDrive\", \"platform_type\": \"Driver\"},\n",
    "    {\"platform_id\": 7, \"platform_name\": \"Blinkit\", \"platform_type\": \"Delivery\"},\n",
    "    {\"platform_id\": 8, \"platform_name\": \"Zepto\", \"platform_type\": \"Delivery\"},\n",
    "    {\"platform_id\": 9, \"platform_name\": \"MagicPin\", \"platform_type\": \"Delivery\"}\n",
    "]\n",
    "\n",
    "# Platform analytics generation\n",
    "def generate_platform_analytics_data(record_id, platform_id):\n",
    "    return {\n",
    "        \"record_id\": record_id,\n",
    "        \"platform_id\": platform_id,\n",
    "        \"active_users\": random.randint(500, 5000),\n",
    "        \"completed_gigs\": random.randint(100, 5000),\n",
    "        \"cancellation_rate\": round(random.uniform(0, 0.2), 2),\n",
    "        \"average_earnings_per_gig\": round(random.uniform(50, 5000), 2),\n",
    "        \"average_completion_time\": random.randint(10, 180),\n",
    "        \"demand_fluctuation_score\": round(random.uniform(0.5, 1.5), 2)\n",
    "    }\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(data, filename, fieldnames):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Generate platform analytics with progress bar\n",
    "print(\"Generating platform analytics data...\")\n",
    "platform_analytics = [\n",
    "    generate_platform_analytics_data(record_id, platform_data[\"platform_id\"])\n",
    "    for record_id, platform_data in tqdm(enumerate(platforms_data, start=1), desc=\"Platform Analytics\")\n",
    "]\n",
    "\n",
    "# Save platform analytics to CSV\n",
    "print(\"Saving platform analytics to CSV...\")\n",
    "save_to_csv(platform_analytics, \"PlatformAnalytics.csv\", list(platform_analytics[0].keys()))\n",
    "\n",
    "print(\"Additional data generation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from 'PlatformAnalytics.csv' has been successfully pushed to MongoDB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Load environment variables\n",
    "# MongoDB Atlas connection string\n",
    "uri = f\"mongodb+srv://arora707vicky:{password}@gigs.7jzwe.mongodb.net/?retryWrites=true&w=majority&appName=gigs\"\n",
    "client = MongoClient(uri)\n",
    "db = client['gig']\n",
    "platform_analytics_collection = db['Platform Analytics']\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = \"PlatformAnalytics.csv\"  # Replace with your actual file path if different\n",
    "\n",
    "# Load data from CSV and push to MongoDB\n",
    "def push_csv_to_mongodb(csv_path, collection):\n",
    "    with open(csv_path, mode='r', newline='', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        data = [row for row in reader]  # Read all rows as dictionaries\n",
    "        for row in data:\n",
    "            # Convert numeric fields to appropriate types\n",
    "            row['record_id'] = int(row['record_id'])\n",
    "            row['platform_id'] = int(row['platform_id'])\n",
    "            row['active_users'] = int(row['active_users'])\n",
    "            row['completed_gigs'] = int(row['completed_gigs'])\n",
    "            row['cancellation_rate'] = float(row['cancellation_rate'])\n",
    "            row['average_earnings_per_gig'] = float(row['average_earnings_per_gig'])\n",
    "            row['average_completion_time'] = int(row['average_completion_time'])\n",
    "            row['demand_fluctuation_score'] = float(row['demand_fluctuation_score'])\n",
    "            collection.insert_one(row)  # Insert each row as a document\n",
    "\n",
    "    print(f\"Data from '{csv_path}' has been successfully pushed to MongoDB.\")\n",
    "\n",
    "# Push data\n",
    "push_csv_to_mongodb(csv_file_path, platform_analytics_collection)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
